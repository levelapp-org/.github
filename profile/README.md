![LevelApp Cover](https://raw.githubusercontent.com/levelapp-org/.github/main/profile/assets/cover-levelapp.jpg)


# üåê LevelApp ‚Äì AI Evaluation Framework for Continuous Testing

**LevelApp** is a modular, open-source framework for evaluating AI systems.  
Think of it as **unit testing + benchmarks for AI products** ‚Äî scenario-driven, model-graded, CI/CD-native.

---

## ‚ú® Features
- ‚úÖ Validate model upgrades before release  
- ‚úÖ Catch regressions in conversations, intents, or outputs  
- ‚úÖ Automate testing with real-world multi-turn scenarios  
- ‚úÖ Score results using LLMs (OpenAI, Claude, IONOS, etc.)  
- ‚úÖ GitHub Action for CI/CD integration  

---

## üß© Core Modules
- **Scenario Builder** ‚Äì craft structured test conversations  
- **Batch Runner** ‚Äì orchestrate evaluations at scale  
- **Report Viewer** ‚Äì analyze results via web UI or API  
- **GitHub Action** ‚Äì run checks directly in your PRs  

---

## ü§ù Contributing
We üíú contributions! PRs, feedback, and examples are welcome.  
Let‚Äôs make AI evaluation as easy and reliable as unit testing.

---

## üì¨ Contact
- Issues ‚Üí [GitHub Issues](https://github.com/levelapp-org)  
- Email ‚Üí opensource@norma.dev  
- Website ‚Üí [norma.dev](https://norma.dev)  

---

## üìÑ License
Released under the [MIT License](./LICENSE).

---

> Built with ‚ù§Ô∏è by [Norma](https://norma.dev) ‚Ä¢ Empowering AI teams with reliable evaluation tools
